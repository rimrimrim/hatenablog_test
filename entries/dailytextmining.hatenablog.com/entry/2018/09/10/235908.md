---
Title: 2018/09/07【76日目】RNNのリカレント層を見ていく
Category:
- 180日間テキストマイニング
Date: 2018-09-10T23:59:08+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/09/10/235908
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132626545355
---

なんだかんだ、ここ1ヶ月くらい追っているRNNの実装のところをみていきます。

すごい単純にいうとRNNは下記のことをやっているだけらしいです。

```python
state_t = 0 #時間Tのすべて0の状態
for input_t in input_sequence:
    output_t = f(input_t, state_t)
    satate_t = output_t
```

要はRNNは、1つ前の繰り返しで計算された数値を再利用するforループだそうです。

もちろんkerasでも実装できるようです。
```python
from keras.layers import SimpleRNN
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(16))
model.summary()
```
実行結果。
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, None, 32)          320000    
_________________________________________________________________
simple_rnn_2 (SimpleRNN)     (None, 16)                784       
=================================================================
Total params: 320,784
Trainable params: 320,784
Non-trainable params: 0
_________________________________________________________________
```

こんな風に層を重ねられるそうです。kerasで層を積み上げるには、完全な出力シーケンス（3次元テンソル）を返す必要があるので、return_sequences=Trueを指定する必要があるみたいです。
```
from keras.layers import SimpleRNN
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(16, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.summary()
```

とは言え、ここまで単純にすると勾配消失問題によって、最終的に訓練が不可能になるそうです。
その問題を解決したのがLSTMだそうです。LSTMは時間刻みにまたがって情報を運ぶためのキャリートラックを追加しており、過去の情報を跡形再注入できるようにして勾配消失問題に対処するとのこと。

自然言語処理でよく使われるRNNは、双方向RNN（bidirectional RNN）。双方向RNNは時間順に見ていくパターンと、逆の順序で見ていくパターンの2方向からパターンを検出し、マージしてから学習をするものだそうです。双方向RNNを使うには下記のようにBidirectional層を追加するみたいです。

```
from keras.preprocessing import sequence
from keras import layers
from keras.models import Sequential
from keras import backend as K

K.clear_session()
model = Sequential()
model.add(layers.Embedding(max_features, 32))
model.add(layers.Bidirectional(layers.LSTM(32)))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)
```

なんかこの辺りにきて、『ゼロから作るDeeo Learning 2』の内容を思い出してきました。
とりあえず今日はこんなところで。

##今日の結果
今日のAKBの呟きは58件でした。
要約するとこんな感じです。

```
珠 理奈 さん 、 、 M ステ で 見 て  た 。 。 本当に おかえりなさい … ✨ いつ まで も ずっと 尊敬 し て  。 大好き  … ！！" " Twitter を 初めて 今日 で 1 年 ✨ まだ 公式 マーク は つい て い ませ ん が … 笑 たくさん の 方 が フォロー し て くださっ て て 、 本当に 嬉しい  。 ありがとう ござい  ！ これから も たくさん 呟い て いく ので 、 ハート と リツイート 待っ て  ?… " " 行っ て き  ✈ ️ ??
"# スリーアウトサヨナラ 篇 3 日 目 ありがとう ござい  た ☺ ️ ❤ ️ 今日 は お昼 公演 に 茂木 さん な ぁなさんうめたん 夜 公演 に えり い と れい ちゃん が 見 に 来 て くれ  た ?❤ ️ 忙しい 中 ありがとう ござい  ?‍♀ ️ ！ 残り 2 日 、 、 頑張 … " " みなさん ！ ありがとう ござい  ！ まだ 中間 発表  が 嬉しい  ✨ この まま 、 もっと 上 の 順位 に いける よう に 最後 まで 応援 よろしく お願い し  ！ ！ " " まき ちゃん の 舞台 「 スリー アウト 〜 サヨナラ 編 〜 」 を 西川 と 見 て き  た 〜 ??
```
```
'嬉しい': 11, '可愛い': 7, 'よい': 3, '楽しい': 3, '遅い': 2, '良い': 2, '長い': 1, '多い': 1, '美味しい': 1, 'やばい': 1, '面白い': 1, '忙しい': 1, 'たのしい': 1, '無い': 1, '凄い': 1, 'いい': 1, '短い': 1, 'とてつもない': 1
'今日': 14, '公演': 12, 'さん': 12, '日': 12, '嬉しい': 11, '舞台': 10, '出演': 10, 'ステ': 9, 'センチメンタルトレイン': 9, '可愛い': 7, '学園': 7, 'チケット': 7, '先行': 7, 'チーム': 6, 'よう': 6, 'エーケービーフォーティーエイト': 6, 'ちゃん': 6, '発売': 6, 
'する': 36, '見る': 16, '今日': 14, '公演': 12, 'さん': 12, '日': 12, 'せる': 12, 'いる': 12, '嬉しい': 11, '舞台': 10, '出演': 10, 'くださる': 10, 'ステ': 9, 'センチメンタルトレイン': 9, 'くる': 8, '可愛い': 7, '学園': 7, 'チケット': 7, '先行': 7, 'いただく': 7, 'くれる': 7,
```
[f:id:rimt:20180910235823p:plain]

[f:id:rimt:20180910235847p:plain]
