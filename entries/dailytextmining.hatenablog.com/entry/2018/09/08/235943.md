---
Title: 2018/09/05【73日目】畳み込みニューラルネットワークの勉強していく
Category:
- 180日間テキストマイニング
Date: 2018-09-08T23:59:43+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/09/08/235943
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132624409828
---

この頃完全にニューラルネットワークについて調べている感じですが、今日もその続きです。

以下、勉強メモです。

畳み込みニューラルネットワーク（CNN）はconventと呼ばれている。
基本的なインスタンス化
```python
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation= 'relu', input_shape=(28,28,1)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation= 'relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation= 'relu'))
```
activationは活性化関数。機能的な仮設空間を作るのに必要で、ReLUが最もよく使われる。
ReLU（Rectified Linear Unit：正規化線形関数）は次のような式で表す。
f(x)=max(0,x)
マイナスの数字を０にするらしい。

これでサマリーを呼び出すと、学習した形が見えるらしいです。
```python
model.summary()
```
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
=================================================================
Total params: 55,744
Trainable params: 55,744
Non-trainable params: 0
_________________________________________________________________
```
(None, 26, 26, 32) のところが3次元のテンソル（height, width, channels）。
3次元だと扱いにくいので、1次元に平坦化する必要があるらしい。

```python
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
```

これですと、TypeError: softmax() got an unexpected keyword argument 'axis’というエラーが起こる。最新のKerasですと、axisをサポートしていないようです。掲示板でさんざん意味がわからないと言われていますが古いバージョンに戻す必要があるそうです。

https://github.com/keras-team/keras/issues/9621

```
sudo pip uninstall keras
sudo pip install keras==2.1.3
```
ちなみにkerasのバージョンを確認するにはjupyter notebook上で、次のコードを実行する必要があるそうです。
```python
import keras
print(keras.__version__) 
```
実行結果。
```
2.1.3
```
これで、１次元に平坦化できたっぽいです。
```python
model.summary()
```
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     
_________________________________________________________________
flatten_1 (Flatten)          (None, 576)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                36928     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 93,322
Trainable params: 93,322
Non-trainable params: 0
_________________________________________________________________
```
flatten_1 (Flatten)          (None, 576)のところが平坦化されたベクトルです。
ちなみにDenseというのは「密集」という意味で密結合された層のことだそうです。

まとめると、畳み込みニューラルネットワークは2つの空間軸（幅と高さ）と1つの深さ（チャネル）に基づき特徴マップと呼ばれる3次元テンソルに対して実行されるようです。


##今日の結果
今日のAKBの呟きは72件でした。
要約するとこんな感じでした。

```
自由 に やら せ て くださっ た AKB の スタッフ さん 、 メンバー 、 ファン の 皆さま 、 友人 、 家族 。 本当に ありがとう 。 これから の 私 も よろしく お願い し  ！ " " 舞台 # スリーアウトサヨナラ 篇 初日 終わり  た ！ ！ ！ 来 て くださっ た 方 ありがとう ござい  た ?‍♀ ️ ❤ ️ 明日 から も 皆さん お待ち し て い  ！ ！ カーテンコール も 本当に ありがとう ござい  ！ ！ 楽しかっ た  ☺ ️ ✨ " " 5 日 19 : 30 ~ 「 スリー アウト ！~ サヨナラ 篇 ~」 昨日 から の 台風 の 影響 で 配送 に 遅れ が 生じ て おり 、 公演 パンフレット の 納品 が 本日 に 間に合わ ない らしい  ?
# はんなり ギロリ の 頼子 さん " " ほんとに ー ！ アカリン 一緒 で よかっ た ！ ！ ありがとう ☺ ️ " " みなさん の 台風 の 影響 、 心配  。 私 は 昨日 京都 で ロケ を し て い て ロケ の 途中 で 停電 し て しまい 照明 さん に 救わ れ 、 日帰り の 予定 が 東京 に は 帰れ ず でし た が 、 夜 の うち に アカリン と 名古屋 まで 車 で 送っ て もらい 朝 東京 へ 。 今 収録 を 終え  た ！ … " " 9 期 の 中 で 妹 的 存在 のみ ゆ ちゃん 。 み ゆ の 存在 に 、 歌声 に 癒さ れ 、 かけ て くれる 言葉 に 何 回 も 励まさ れ た よ ！ 本当に ありがとう ！ ！ 自由 に 、 み ゆ の やり たい こと を どんどん やっ て ほしい なっ て 思い  ！ 事前 に 聞い て は い た けど 、 はやく 会 … " " ドラゴン 役 で 出演 する こと に なり  た ！！?
1000 % と タシマンナ も 聴け て 嬉しかっ た  ?← ただ の ファン # 프로듀스 48 " " 舞台 『# マジムリ 学園 』 荒地 工業 高校 ゾンビ 役 として 出演 さ せ て 頂く こと に なり  た ?‍♀ ️ マジムリ 学園 出演 でき て 嬉しい なぁ そして 今回 は 結構 見た目 も … ゾンビ に なる の が 今 から 楽しみ ー 日本 青年 館 ホール 10 月 19 日 〜 ぜひ … " " 今日 は # AKBINGO の 収録 でし た ❗ ️ ❗ ️ 初めて ______ を やり  た 周り の 反応 は 微妙 だ けど 面白かっ た  ?
```

```
'可愛い': 7, '嬉しい': 7, 'すごい': 6, '寂しい': 4, '優しい': 4, 'ない': 3, '面白い': 3, 'いい': 3, '寒い': 2, 'よい': 2, 'ほしい': 2, '強い': 1, '素晴らしい': 1, '眠い': 1, 'うれしい': 1, '早い': 1, '楽しい': 1, '上手い': 1, 'やさしい': 1, '悲しい': 1, 'あやい': 1, '大きい': 1, 'はやい': 1, '温かい': 1, 'やすい': 1, 'っぽい': 1}
'さん': 17, 'こと': 13, '今日': 12, '学園': 10, '私': 10, 'ゆ': 10, 'エーケービーフォーティーエイト': 10, '大好き': 9, '出演': 8, '収録': 8, '可愛い': 7, '嬉しい': 7, 'すごい': 6, '皆さん': 6, '発売': 6, '歌': 6, '日': 6, '舞台': 5, '発表': 5, '明日': 5, '昨日': 5, '卒業': 5, '時間': 5, '一緒': 5, 'お願い': 5, '方': 5,
'する': 55, 'さん': 17, 'くださる': 16, 'こと': 13, '今日': 12, 'なる': 12, 'いる': 12, '学園': 10, '私': 10, 'ゆ': 10, 'エーケービーフォーティーエイト': 10, '見る': 10, '大好き': 9, 'てる': 9, 'せる': 9, '出演': 8, '収録': 8, 'れる': 8, '可愛い': 7, '嬉しい': 7, 'すごい'
```
[f:id:rimt:20180908235937p:plain]

[f:id:rimt:20180908235909p:plain]
