---
Title: 2018/7/26【33日目】ブログをクローリングさせるその3
Category:
- 180日間テキストマイニング
Date: 2018-07-28T17:38:05+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/07/28/173805
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132605361075
---

昨日の段階でリンクできるURLを取得できましたが、ブログのエントリーじゃないurlを取得していますので、エントリーだけを取得したいと思います。

リスト形式の中からある文字を取得するには、if 〇〇 in リストとすれば良いので、こんな感じで取得できると思います。

```python
links =[]
for link in soup.find_all('a'):
    url = link.get('href')
    if "entry" in url:
        links.append(url)
```

```
TypeError: argument of type 'NoneType' is not iterable
```

エラーが出ました。Noneはイテラブルではないとのこと。どうやらリストの中にNoneが含まれていて、それがエラーを起こしているようです。is not Noneという条件を付け足して、Noneを無視するようにします。なお、Noneを比較する際はisを使うことを推奨されているようです。

```python
links =[]
for link in soup.find_all('a'):
    url = link.get('href')
    if url is not None and "/saho-iwatate/entry-" in url:
        links.append(url)
links
```
実行結果。
```
['/saho-iwatate/entry-12392720547.html?frm=theme',
 '/saho-iwatate/entry-12392720547.html?frm=theme',
 '/saho-iwatate/entry-12392720547.html?frm=theme#cbox',
 '/saho-iwatate/entry-12389250032.html?frm=theme',
 '/saho-iwatate/entry-12389250032.html?frm=theme',
 '/saho-iwatate/entry-12389250032.html?frm=theme#cbox',
（以下、省略）
```
frm=themeというのも邪魔なので、これもif文で取ってしまいます。

```python
links =[]
for link in soup.find_all('a'):
    url = link.get('href')
    if url is not None and "/saho-iwatate/entry-" in url and "?frm=theme" not in url:
        links.append(url)
links
```
実行結果。
```
['/saho-iwatate/entry-12392720547.html',
 '/saho-iwatate/entry-12389250032.html',
 '/saho-iwatate/entry-12387116933.html',
 '/saho-iwatate/entry-12386645732.html',
 '/saho-iwatate/entry-12385696041.html',
 '/saho-iwatate/entry-12384984890.html',
 '/saho-iwatate/entry-12383158282.html',
 '/saho-iwatate/entry-12376543915.html',
 '/saho-iwatate/entry-12374361368.html',
 '/saho-iwatate/entry-12370071817.html']
```
うまくブログのエントリーだけ取得できました。

各エントリーのタイトルはclass='skinArticleTitle’にあるそうなので、これを指定して、stringを取得すればいけるような気がします。

```python
for urls in links:
    entry_url = "https://ameblo.jp/" + urls
    req = urllib.request.urlopen(entry_url)
    soup = BeautifulSoup(req.read(), "lxml")
    # print("title.string：" + str(soup.title))
    print(soup.find(class_='skinArticleTitle').string)
```
```
大事なこと
はむっと
まさか…
おもちゃの夏
朝なのか夜なのか
世界選抜総選挙
君は僕の風
劇場3days
強く！
土曜日
```
あとは必要なのは日付と本文なので、こんな感じになりました。

```python
for urls in links:
    entry_url = "https://ameblo.jp/" + urls
    req = urllib.request.urlopen(entry_url)
    soup = BeautifulSoup(req.read(), "lxml")
    # print("title.string：" + str(soup.title))
    print(soup.find(class_='skinArticleTitle').string)
    print(soup.find(class_='skin-textQuiet').string)
    print(soup.find(class_ = 'skin-entryBody').text)
```

あとはこの情報をDBに入れていけば良さそうです。

## 今日の結果
今日のAKBの投稿数は55件でした。
[f:id:rimt:20180728173747p:plain]

```
'嬉しい': 12, '楽しい': 9, 'すごい': 4, 'ない': 3, '可愛い': 3, '早い': 3, '遅い': 2, 'いい': 2, '凄い': 2, 'よい': 2, '果てしない': 1, 'もどかしい': 1, 'たのしい': 1, 'すっごい': 1, '美味しい': 1, 'カッコイイ': 1, '明るい': 1, 'にくい': 1, 'よろしい': 1, '遠い': 1, '涼しい': 1, '仲良い': 1, '五月蝿い': 1, '良い': 1, 'かわいい': 1, 'かわいらしい': 1, '熱い': 1, '難しい': 1, 'っぽい': 1, '長い': 1, 'つまんない': 1})
嬉しい': 12, 'ちゃん': 10, '楽しい': 9, 'さん': 9, 'イベント': 8, '日': 8, 'エーケービーフォーティーエイト': 8, '笑': 7, '人': 7, 'こと': 7, '一緒': 7, '時': 7, '選抜': 7, '時間': 7, '今年': 6, 'ユニット': 6, '私': 6, 'フレッシュ': 6, '出演': 6, '夏': 6, 'そう': 6,
'する': 33, 'なる': 14, '嬉しい': 12, 'くださる': 12, 'ちゃん': 10, '楽しい': 9, 'さん': 9, '来る': 9, 'イベント': 8, '日': 8, 'エーケービーフォーティーエイト': 8, '行く': 8, 'いる': 8, '笑': 7, '人': 7, 'こと': 7, '一緒': 7, '時': 7, '選抜': 7, '時間': 7, 'せる': 7, 
```
