---
Title: 2018/7/25【32日目】Pythonでブログをクローリングさせる
Category:
- 180日間テキストマイニング
Date: 2018-07-26T23:58:07+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/07/26/235807
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132604892032
---

昨日はWebスクレイピングをしてみましたが、プログラムで次から次へとサイトを見ていくのを<b>クローリング</b>というらしいです。

今日は昨日は引き続いて、クローリングをしてみたいと思います。

## クローリングの実装
昨日に引き続いてAKB48 岩立沙穂のアメブロで試してみます。ただ、トップページより、記事一覧の方がクローリングしやすそうなので、記事一覧ページで行ってみます。URLはこちらです。

```
https://ameblo.jp/saho-iwatate/entrylist.html
```
ひとまず昨日のコードに入れてみます。
```python
import urllib.request

req = urllib.request.urlopen('https://ameblo.jp/saho-iwatate/entrylist.html')
soup = BeautifulSoup(req.read(), "lxml")

print("a：" + str(soup.h3))
for link in soup.find_all('h3'):
    print(link.get('text'))
```
```
UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.
```
というエラーが。。。エラー内容を見てみると、パーサーを指定していないから、こっちで"lxml”を指定したよっていう忠告ですね。"lxml”が一番優れていると書いてありますので、"lxml”をちゃんと指定します。

エラーも解決できそうなので、まずはh3タグのテキストを取得してみます。

```python
import urllib.request

req = urllib.request.urlopen('https://ameblo.jp/saho-iwatate/entrylist.html')
soup = BeautifulSoup(req.read(), "lxml")

print("a：" + str(soup.h3))
for link in soup.find_all('h3):
    print(link.get('text'))
```

```
a：<h3 class="skin-widgetTitle" data-uranus-component="widgetTitle" lang="en">Profile</h3>
None
None
None
None
None
```

うまく取得できていないですね。
時間がないのでその2に続きます。
