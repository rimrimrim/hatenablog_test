---
Title: 2018/7/24【32日目】ブログからテキストを抜き出す
Category:
- 180日間テキストマイニング
Date: 2018-07-26T02:54:24+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/07/26/025424
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132604611649
---

12時前に家に帰れなかったので、日にちを跨いでいるのですが24日分をアップします。

このブログでは1日目からずっとTwitterからテキストデータを保存していましたが、AKBのメンバーはブログもやっているようなので、ブログからもテキストを保存できるようにしたいと思います。

今回スクレイピングをするのは、AKB48 岩立沙穂です（ググったら一番最初に出てきました）
https://ameblo.jp/saho-iwatate/

[https://ameblo.jp/saho-iwatate/:embed:cite]



## スクレイピング
Webからデータを取得することをWebスクレイピングというらしいですが、Pythonの場合はBeautifulsoupというライブラリを使用すれば、手軽にできるそうです。

ドキュメントがありますので、こちらに沿って、Beautifulsoupを使用していきたいと思います。

https://www.crummy.com/software/BeautifulSoup/bs4/doc/

まずはpip installから。

```
pip install beautifulsoup4
```
インストールできましたら、コード中でimportします。

```python
from bs4 import BeautifulSoup
```
importできたらドキュメント通りにコードを実行。
```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')

print(soup.prettify())

```
できませんでした。
```
UserWarning: "https://ameblo.jp/saho-iwatate/" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.
  ' that document to Beautiful Soup.' % decoded_markup
```
BeautifulSoupはHTTPクライアントじゃないから、URLを読み込めないよというエラー。

昨日のPDFを読み込む際に少し出てきましたが、parserというものに変換しなければならないようです。htmlparserを取得するには、urllib.requestでできるようです。

```python
import urllib.request

req = urllib.request.urlopen('https://ameblo.jp/saho-iwatate/')
soup = BeautifulSoup(req.read())

print(soup.prettify())
```
実行結果。
```python
<!DOCTYPE html>
<html class="columnB fixed" data-base-skin-code="uranus" data-skin-code="ur_std_official_template14_en" lang="ja">
 <head>
  <meta charset="utf-8"/>
  <meta content="origin" name="referrer"/>
  <meta content="AKB48 岩立沙穂さんのブログです。最近の記事は「大事なこと（画像あり）」です。" data-react-helmet="true" name="description"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <style media="screen,print">
（いか、省略）
```

ページの全テキストが取得できたので、あとは、HTMLタグを頼りに色々取得できるみたいです。
```python
print("title：" + str(soup.title))
print("title.name：" + str(soup.title.name))
print("title.string：" + str(soup.title.string))
print("parent.name：" + str(soup.title.parent.name))
print("p['class']：" + str(soup.p['class']))
print("a：" + str(soup.a))
```
```
title：<title data-react-helmet="true">AKB48 岩立沙穂オフィシャルブログ「発声練習、はじめま～す♪」Powered by Ameba</title>
title.name：title
title.string：AKB48 岩立沙穂オフィシャルブログ「発声練習、はじめま～す♪」Powered by Ameba
parent.name：head
p['class']：['skin-blogSubTitle']
a：<a name="pageTop"></a>
```
for文で回すこともできて、リンクを取得することもできるようです。
```python
for link in soup.find_all('a'):
    print(link.get('href'))
```
```
http://www.ameba.jp/
http://mypage.ameba.jp/
http://pigg.ameba.jp?frm_id=c.pc-inner-header-blog-pigghome
https://blog.ameba.jp/ucs/top.do
https://official.ameba.jp
https://ameblo.jp
（以下、省略）
```
おそらく、ここで得たurlを利用して、次から次へとスクレイピングできるんだと思います。

とりあえずの使い方はここまで。
明日はもう少し詳しく見ていきたいと思います。

##今日の結果
今日の呟きは70件でした。どうしたのでしょうかね？
[f:id:rimt:20180726025254p:plain]

```
'可愛い': 6, '楽しい': 3, '良い': 3, '暑い': 2, 'すごい': 2, '嬉しい': 1, '優しい': 1, '新しい': 1, '恥ずかしい': 1, 'ない': 1})
'ちゃん': 10, '可愛い': 6, 'さん': 6, '公演': 6, 'こと': 5, '今日': 5, 'アクシュカイ': 5, '日': 5, 
'する': 13, 'ちゃん': 10, '頑張る': 7, '可愛い': 6, 'さん': 6, '公演': 6, 'なる': 6, 'こと': 5, '今日': 5, 'アクシュカイ': 5, '日': 5,
```
どうやら特別公演が今日から始まるようですね。
