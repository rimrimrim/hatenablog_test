---
Title: 2018/7/10【18日目後半】MeCabに登録した辞書の影響を見てみる
Category:
- 180日間テキストマイニング
Date: 2018-07-11T19:39:39+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/07/11/193939
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132600247215
---

MeCabに単語を登録したので、その影響を見ていきたいとおもいます。
まずは今日のAKBメンバーによる呟きから確認してみます。

##今日の結果
今日の結果は37件でした。
呟かれたのはこんな感じです。
```
'嬉しい': 4, '楽しい': 3, 'すごい': 2, '多い': 2, '良い': 2, 'あおい': 2, '優しい': 2, '美味しい': 1, 'いい': 1, '仲良い': 1, '欲しい': 1, '暑い': 1, 'よろしい': 1, '高い': 1, '寂しい': 1, '忙しい': 1, '面白い': 1})
'皆さん': 12, '今日': 8, 'さん': 8, '配信': 6, '人': 5, '私': 5, '笑': 5, '公演': 5, '嬉しい': 4, '写真': 4, 'アイドル': 4, 'みんな': 4, '自主': 4, '練': 4, 'さ': 4,
'する': 30, '皆さん': 12, 'いる': 9, '今日': 8, 'さん': 8, 'れる': 7, 'くれる': 7, '配信': 6, '来る': 6, 'なる': 6, 'くる': 6, '人': 5, '私': 5, '笑': 5, '公演': 5, 'くださる': 5, '頑張る': 5, '嬉しい'
```
これだけですとテキスト量が少なすぎますので、直近5日分の呟きを追加します。
すると、こんな感じの画像ができました。

[f:id:rimt:20180711195505p:plain]

次にMeCabに単語を登録して実行してみます。
追加した単語はこんな感じです。

```csv
AKB48,*,*,1,名詞,固有名詞,組織,*,*,えーけーびーふぉーてぃーえいと,エーケービーフォーティーエイト,エーケービーフォーティーエイト
13期,*,*,1,名詞,固有名詞,組織,*,*,じゅうさんき,ジュウサンキ,ジュウサンキ
ON8+1,*,*,1,名詞,固有名詞,一般,*,*,おーえぬえいと,オーエヌエイト,オーエヌエイト
こみの部屋,*,*,1,名詞,固有名詞,一般,*,*,おーえぬえいと,オーエヌエイト,オーエヌエイト
（中略）
神7,*,*,1,名詞,固有名詞,一般,*,*,かみせぶん,カミセブン,カミセブン
センチメンタルトレイン,*,*,1,名詞,固有名詞,一般,*,*,せんちめんたるとれいん,センチメンタルトレイン,センチメンタルトレイン
チームA,*,*,1,名詞,固有名詞,組織,*,*,ちーむえー,チームエー,チームエー
TeamA,*,*,1,名詞,固有名詞,組織,*,*,ちーむえー,チームエー,チームエー
```

このcsvをコンパイルして、もう一度画像を作ってみます。
こんな画像になりました。
[f:id:rimt:20180711193305p:plain]

なかなか分かりにくいですが、新しく「AKB48」と「レッツゴー研究生」という単語が含まれてますね。

##さらに影響を見てみる
5日分で2つの単語が新しく出てきたということは、もっとテキスト量が多いとかなりの影響が出そうです。
試しに15日目に行ったtf-idf分析のコードにこの辞書を追加してみます。コード上で辞書を追加するには-uオプションを使えばよいそうです。こんな感じになりました。

```python
for content in os.listdir('test'):
    with open('test/' + content, 'r') as read_file, open('henkan/' + content, 'w') as write_file:
        for line in read_file:
            reline = re.sub('(https?)(:\/\/[-_.!~*\'()a-zA-Z0-9;\/?:\@&=+\$,%#]+)', '', line)
#辞書を追加
            tagger = MeCab.Tagger('mecabrc -u /Users/user_name/py_sql/akb_dic/akb.dic')
            tagger.parse('')
            node = tagger.parseToNode(reline)
            while node:
                if node.feature.startswith('名詞'):
                    write_file.write(node.surface + '\n')
                node = node.next
```

CountVectorizerで単語を数値化し、数を数えてみます。

```python
count_vectorizer = CountVectorizer(input='filename')
files = ['henkan/' + content for content in os.listdir('test')]
X_train_counts = count_vectorizer.fit_transform(files)
X_train_counts.shape
```
実行結果。
```
(30, 2601)
```
辞書を登録しないと（30, 2583)でしたので、単語数がかなり増えていることになります。
次は、辞書登録前と辞書登録後でスコアが高い単語を見てみます。

####辞書登録後
```
'公演', 0.3331445293910642
'さん', 0.30934849157741678
'今日', 0.26175641595012189
'村山', 0.214164340322827
'チーム4', 0.19036830250917955
'写真', 0.1665722646955321
'初日', 0.1665722646955321
'撮影', 0.1665722646955321
'皆さん', 0.1665722646955321
'showroom', 0.14277622688188465
```
####辞書登録前
```
('公演', 0.33016664144560476)
('さん', 0.30658330991377586)
('今日', 0.259416646850118)
('チーム', 0.21224998378646021)
('村山', 0.21224998378646021)
('写真', 0.16508332072280238)
('初日', 0.16508332072280238)
('撮影', 0.16508332072280238)
('皆さん', 0.16508332072280238)
('showroom', 0.14149998919097348)
```
チームという言葉がチーム4に変わったことにより、’村山’と'チーム'の順位が入れ替わっていますね。これは今までのチームKだとかチームAだとかが同じ’チーム’という単語として認識され無くなったことで、出現回数が減ったのだと思われます。ちなみに辞書ありのトップ10以降は下記の通りです。AKB48が11位、akbingoが12位にランクインしています。

```
'akb48', 0.095184151254589774
'akbingo', 0.095184151254589774
'あと', 0.095184151254589774
'お願い', 0.095184151254589774
'こと', 0.095184151254589774
'ちょ', 0.095184151254589774
'ひな', 0.095184151254589774
'みなさん', 0.095184151254589774
'みんな', 0.095184151254589774
'メンバー', 0.095184151254589774,
```

当然かもしれませんでしたが、なかなか影響がありそうですね。
これからしっかり辞書を作っていきたいと思います。

##今日勉強したこと
- MeCabへユーザー辞書を登録する方法
- 辞書の影響力の大きさ


明日からポジティブネガティブ判定を勉強していきたいと思います。
