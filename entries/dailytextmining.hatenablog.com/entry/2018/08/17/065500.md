---
Title: 2018/8/16【54日目】夏休みだし『ゼロから作るDeep  Learning 2 自然言語処理編』を読んでいく
Category:
- 180日間テキストマイニング
Date: 2018-08-17T06:55:00+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/08/17/065500
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132611515626
---

夏休みですので、読書でもしたいと思います。今回読むのは『ゼロから作るDeep  Learning 2 自然言語処理編』です。

以下、ただの読書メモです。

##読書メモ
単語の意味を理解させるのが重要。コンピューターに単語を理解させるのには下記の方法がある。
シソーラスによる手法
カウントベースの手法
推論ベースの手法（word2vec）

シソーラスというのは類義語辞書のこと。自然言語処理において最も有名なシソーラスはWordNet。シソーラスは以下のような弱点がある。
時代の変化に対応するのが困難
人の作業コストが高い
単語の細かなニュアンスを表現できない

カウントベースはある単語において、その周囲にどのような単語がどれだけ出現するかをカウントこと。

単語IDの作り方
```
word_to_id = {}
id_to_word = {}

for word in words:
	if word not in word_to_id:
		new_id = len(word_to_id)
		word_id[word] = new_id
		id_to_word[new_id] = word
```
len関数を使ってidを振る。

自然言語処理においてコンテキスト（文脈）とは周辺の単語という使い方をする。このコンテキストのサイズのことをウィンドウサイズといい、ウィンドウサイズ1だと、ある単語の左右1単語のことを指す。これが分散表現となる。

ベクトル間の類似度を測るにはいろいろな方法があるがコサイン類似度がよく用いられる。

コサイン類似度の計算は次の通り。
```
def cos_similarity(x,y):
    nx = x / np.sqrt(np.sum(x**2) + eps)
    ny = y / np.sqrt(np.sum(y**2) + eps)
    return np.dot(nx, ny)
```
epsというのはイプシロンのことで、とても小さな数のこと。計算にはほとんど関係ないが1 / 0対策に必要。

np.dotというのはドット積のことで、ベクトル演算の一種。2つの同じ長さの数列から一つの数値を返す演算のこと。

このままベクトルを使うと、0の要素が多くなってしまい、無駄に何十万次元のデータができてしまう。なので、重要な要素だけを残し、次元削減することを特異値分解（SVD）という。SVDの計算は次の通り。

X = USV^T
X行列をUとSとVに分解する。
コードで書くと次の通り。
```
U, S, V = np.linalg.svd(w)
```

推論ベースというのはword2vecのなかのCBOWモデルのこと。周辺の単語からターゲトとなる単語を推測する手法。カウントベースでは、一度に大量の統計データを読み込ませるため、計算処理が莫大になる。推論ベースだと学習を小分けにして学習データを更新できる。

word2vecに搭載されているもう1つのモデルであるスキップグラムモデルは、CBOWモデルとは逆にターゲットから周辺の単語を推論する手法。

スキップグラムの方が良い精度を持つが、出力層が2つ以上になるので、計算処理に時間がかかる。
自然言語処理において分散処理が大事なのは、転移学習ができるから。ある分野で学んだことは、他の分野にも適用できる。

##今日の結果
今日のAKBメンバーによる呟きは52件でした。
今日は特に何もなさそうな日ですね。
[f:id:rimt:20180817020459p:plain]

```
{'嬉しい': 7, '楽しい': 5, 'すっごい': 2, 'いい': 2, '悪い': 2, 'かわいい': 1, '可愛い': 1, 'おしい': 1, '宜しい': 1, '多い': 1, '暑い': 1, '長い': 1, '涼しい': 1, 'ない': 1, 'すごい': 1, '難しい': 1, '強い': 1, 'うまい': 1})
'私': 9, '日': 9, 'エーケービーフォーティーエイト': 9, '皆さん': 8, 'さん': 8, '嬉しい': 7, '出演': 7, 'こと': 6, 'ちゃん': 6, 'お願い': 6, 'ビート': 6, '今日': 6, '楽しい': 5, '大好き': 5, 'の': 5, 'カーニバル': 5, 
'する': 40, 'てる': 10, '私': 9, '日': 9, 'エーケービーフォーティーエイト': 9, '皆さん': 8, 'さん': 8, '見る': 8, 'いる': 8, 'なる': 8, 'せる': 8, '嬉しい': 7, '出演': 7, 'く': 7, 'こと': 6, 'ちゃん': 6, 'お願い': 6, 'ビート': 6, '今日': 6, '来る': 6, '頂く': 6, '楽しい': 5,
```
