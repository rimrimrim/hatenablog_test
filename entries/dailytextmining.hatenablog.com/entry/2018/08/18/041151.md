---
Title: 2018/8/17【55日目】夏休みだし『ゼロから作るDeep  Learning 2』を読んでいく2
Category:
- 180日間テキストマイニング
Date: 2018-08-18T04:11:51+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/08/18/041151
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132611849917
---

『ゼロから作るDeep  Learning 2 自然言語処理編』の5章以降であるRNN（リカレントニューラルネットワーク）から再開です。
以下はただの勉強メモです。

##勉強メモ

今までのニューラルネットワークは、フィードフォワードという流れが一方向のネットワーク。これだと時系列データを扱えない。時系列データも考慮できるのがRNN（リカレントニュートラルネットワーク）。recurrentは「循環」という意味。

RNNの特徴は、ループする経路を持ち、過去の情報を持ちつつ、最新の情報を学習する。

言語モデルは単語の並びに対して確率を与える。
マルコフ性は、未来の状態が現在の状態だけに依存して決まることをいう。

大きな時系列データを扱うとネットワークのつながりが大きくなりすぎるので、適当な長さで断ち切る必要がある。小さく切り取ったネットワークをつなげるのがTruncated BPTT。BPTTはBackpropagation Throug Timeという誤差逆伝播法と呼ばれるアルゴリズム。

RNNを使った言語モデルをRNNLM（RNN Language Model）と呼ぶ。
言語モデルの予測性能の良さを評価するにはパープレキシティを使う

RNNを使った言語モデルができると、新たな文章を生成できる。仕組みとしては、最初に開始を知らせる区切り文字を渡し、モデルが次にくる確率が高い単語を選んでくる。

また、RNNLMができるとseq2seq（sequence to sequence：時系列から時系列）が実装できる。seq2seqはRNNとRNNを組み合わせたニューラルネットワーク。入力と出力が時系列データなので学習を更新できる

seq2seqの精度を向上させるにはreverseとpeekが有効。またAttentionという時系列データの対応関係を学習する技術もあり、これは大幅に精度を向上させる。

##感想
4章のword2vecまではなんとかついていけましたが、5章からのRNNはコードレベルでは全く理解できていないです。とはいえRNNが初見なので、こんなものかと思います。本書でRNNの詳しい仕組みが知れてよかったです。

seq2seqもセックトューセックと読んでいたくらいでした。

##今日の結果
今日のAKBの呟きは67件でした。
今日からセンチメンタルトレインの個別握手会の申し込みが始まったそうです。

[f:id:rimt:20180818040530p:plain]

```
'楽しい': 6, '嬉しい': 5, '可愛い': 4, 'ない': 3, 'すごい': 3, 'よい': 2, '幅広い': 2, '面白い': 2, 'いい': 2, 'やばい': 2, '素晴らしい': 1, 'くい': 1, 'いたい': 1, '良い': 1, '近い': 1, '早い': 1, '甘い': 1, '大きい': 1})
'アクシュカイ': 12, '公演': 11, '日': 10, 'さん': 10, 'ちゃん': 9, '中': 8, '方': 8, 'ん': 7, '月': 7, '楽しい': 6, '笑': 6, '人': 6, 'お願い': 6, 'センチメンタルトレイン': 6, '受付': 6, '写真': 6, '皆さん': 6, 'よう': 6, 
'する': 35, 'なる': 18, 'いる': 15, 'くださる': 13, 'アクシュカイ': 12, 'てる': 12, '公演': 11, '日': 10, 'さん': 10, 'ちゃん': 9, '中': 8, '方': 8, '来る': 8, 'ん': 7, '月': 7, 'くる': 7, 'くれる': 7, '会う': 7, 'みる': 7, 
```
