---
Title: 2018/8/8【46日目】Word2vecを試すにはkaggleのチュートリアルが良さそう
Category:
- 180日間テキストマイニング
Date: 2018-08-09T06:55:00+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/08/09/065500
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132608920624
---

昨日、TensolFllowのword2vecのチュートリアルを試してみましたので、一応コードを確認していきます。

##word2vecで画像を作成する際のSTEP

サンプルコード内にコメントでSTEPが書いてありますので、それを集めてみました。

- Step 1: Download the data.
- Step 2: Build the dictionary and replace rare words with UNK token.
- Step 3: Function to generate a training batch for the skip-gram model.
- Step 4: Build and train a skip-gram model.
- Step 5: Begin training.
- Step 6: Visualize the embeddings.

上から順に行きます。
#####1. データをダウンロードする。

まぁ、これはこちらですね。
```python
url = 'http://mattmahoney.net/dc/'

filename = maybe_download('text8.zip', 31344016)
```

#####2.辞書を作る。そして、レアな単語はUNK tokenに置き換える。

UNK tokenというのは未知語を集めたオブジェクトのことぽいです。
辞書はこの辺りで作っているみたいです。

```python
  for word in words:
    index = dictionary.get(word, 0)
    if index == 0:  # dictionary['UNK']
      unk_count += 1
    data.append(index)
  count[0][1] = unk_count
```
unk_countがUNK tokenぽいですね。

#####3,スキップグラムモデルのトレーニングバッチを生成する関数を作る

word2vecには分散表現を実行するための手段として、スキップグラムとCBOWというのがあるらしいです。ここではスキップグラムのトレーニングデータ作成するための関数を実装しているみたいです（どういう処理かは不明）。おそらくこの辺りで行っているはず。わからないですが。

```python
  for i in range(batch_size // num_skips):
    context_words = [w for w in range(span) if w != skip_window]
    words_to_use = random.sample(context_words, num_skips)
    for j, context_word in enumerate(words_to_use):
      batch[i * num_skips + j] = buffer[skip_window]
      labels[i * num_skips + j, 0] = buffer[context_word]
    if data_index == len(data):
      buffer.extend(data[0:span])
      data_index = span
    else:
      buffer.append(data[data_index])
      data_index += 1
  # Backtrack a little bit to avoid skipping words in the end of a batch
  data_index = (data_index + len(data) - span) % len(data)
  return batch, labels


batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
```

#####4,スキップグラムのモデルを実装する
もうこの辺りになりますと、全くわかりませんが、この辺りが怪しいんじゃないかと。。。

```python
# Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))
```

#####5. トレーニングを始める
これはおそらくこの辺りですね。
```python
  for step in xrange(num_steps):
    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,
                                                skip_window)
    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

    # Define metadata variable.
    run_metadata = tf.RunMetadata()
```

6.可視化
可視化は最後のここらへんですね。
```python
  tsne = TSNE(
      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')
  plot_only = 500
  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])
  labels = [reverse_dictionary[i] for i in xrange(plot_only)]
  plot_with_labels(low_dim_embs, labels, 'tsne.png')
```

というわけで、このサンプルでは何をやっているのか全くわかりませんでした。

仕方がないので、他に何かいいチュートリアルはないかなーと探していたのですが、kaggle内に良さそうなものがありました。

それが

<b>Bag of Words Meets Bags of Popcorn</b>

というやつです。


[https://www.kaggle.com/c/word2vec-nlp-tutorial]



## Bag of Words Meets Bags of Popcornについて

kaggleの中にいくつかチュートリアルがあるのですが、Bag of Words Meets Bags of Popcornはword2vecを使うためのチュートリアルの1つです。映画のレビューをword2vecを使って、ポジティブな意見かネガティブな意見かを判定するようです。チュートリアルなだけあってかなり丁寧に解説が書いてありますので、今回こそ期待できそうです。

ひとまず今日は読み込みだけ。
```python
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt

train = pd.read_csv('all/labeledTrainData.tsv',
                    header = 0,
                    delimiter = "\t",
                    quoting = 3)
train.head()
```
実行結果。
```
	id	sentiment	review
0	"5814_8"	1	"With all this stuff going down at the moment ...
1	"2381_9"	1	"\"The Classic War of the Worlds\" by Timothy ...
2	"7759_3"	0	"The film starts with a manager (Nicholas Bell...
3	"3630_4"	0	"It must be assumed that those who praised thi...
4	"9495_8"	1	"Superbly trashy and wondrously unpretentious ...
```
idとsentimentとレビューが記録されているデータのようです。
明日から詳しくやっていきます。

##今日の結果
本日のAKBメンバーによる呟きは45件でした。
'聖': 7, '菜': 7, '福岡': 6,がかなり出ていることから福岡聖菜は変わらず呟きが多いのでしょうね。
[f:id:rimt:20180809014029p:plain]

```
'可愛い': 5, '嬉しい': 4, '楽しい': 4, 'よい': 4, '新しい': 2, 'うれしい': 1, '遅い': 1, 'すごい': 1, '愛しい': 1, '宜しい': 1, 'っぽい': 1, '早い': 1, 'ぽい': 1, '可愛らしい': 1, 'かわいい': 1, '美味しい': 1, '怖い': 1, '少ない': 1, '小さい': 1})
'聖': 7, '菜': 7, '福岡': 6, '可愛い': 5, '配信': 5, '今日': 5, '時': 5, '嬉しい': 4, '楽しい': 4, 'よい': 4, '私': 4, 'さん': 4, '回': 4, '西川': 4, 'ちゃん': 4, '顔': 4, '放送': 4, '友達': 3, '人': 3,
'する': 26, 'くださる': 10, '見る': 8, '聖': 7, '菜': 7, 'なる': 7, '福岡': 6, 'みる': 6, '可愛い': 5, '配信': 5, '今日': 5, '時': 5, 'くれる': 5, 'てる': 5, 'すぎる': 5, 'ある': 5, '嬉しい'
```
