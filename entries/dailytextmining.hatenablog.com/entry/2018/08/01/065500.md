---
Title: 2018/7/31【38日目】scrapyを使いこなして、自動でテキストを集めよう2
Category:
- 180日間テキストマイニング
Date: 2018-08-01T06:55:00+09:00
URL: https://dailytextmining.hatenablog.com/entry/2018/08/01/065500
EditURL: https://blog.hatena.ne.jp/rimt/dailytextmining.hatenablog.com/atom/entry/10257846132606445351
---

scrapyを使えばjson形式で、データを保存できることがわかりましたので、次に進んでいきます。

##リンクを追う
人がサイトからサイトへリンクをたどっていけるように、scrapyもリンクをたどっていろいろな情報にアクセスできるようです。

昨日に続いて、チュートリアルを見ていきます。

###リンクをたどる
まずはaタグ内のリンクを知る必要がありますので、シェルを起動してa要素を取得します。
```
response.css('li.next a').extract_first()
```
実行結果。
```
'<a href="/page/2/">Next <span aria-hidden="true">→</span></a>'
```
aタグ内の要素が取得できました。しかし、本当に必要なのは、hrefの中身の”/page/2/”なのでもう一度シェルにコマンドを打ち込みます。

```
response.css('li.next a::attr(href)').extract_first()
```
引数に::attr(href)というのが追加されました。attrはJavaSceriptで言うと、HTML要素の属性を取得したり設定することができるメソッドだそうです。

実行結果。
```
'/page/2/'
```
hrefの中身が取得できました。

これを利用したサンプルがこちらです。
```
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('span small::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

hrefがNoneになるまでfor文を回すという、なんだか恐ろしげなコードが追加されました。
実際のサイトでこれを実行すると情報量がやばそうですね。わからないですが。

ひとまずサンプルとして、これで実行してみますと、確かに情報がいっぱい保存されています。
```
scrapy crawl quotes -o quotes-humor.json
```
```
[
{"text": "\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d", "author": "Albert Einstein", "tags": ["change", "deep-thoughts", "thinking", "world"]},
（以下は省略）
```

これで大体の基礎は終わりのようです。

##知っておいた方が良さそうなこと
###環境設定
環境設定は［scrapy.cfg］にあるそうです。
と、言っても最初に書いてあるのはこれだけです。
```
[settings]
default = tutorial.settings

[deploy]
#url = http://localhost:6800/
project = tutorial
```
deployと書いてあるくらいですので、サーバーにアップした時に使うんだと思います。

###Scrapyのコマンド
12個しかないらしいです。せっかくなので、ここで一通り調べておきます。

- scrapy startproject <project_name> [project_dir]

	新しい Scrapy プロジェクトを作成する。

- scrapy genspider [-t template] <name> <domain>

	プロジェクトの中に新しいスパイダーを作成する。

- scrapy crawl <spider>

	スパイダーを使ってクローリングさせる

- scrapy check [-l] <spider>

	コントラクトチェックを実行。コントラクトとは訳すると「契約」という意味ですが、プログラムがしっかり動くかどうかを確認することをいうそうです。

- scrapy list

	使えるスパイダーを一覧表示します。

- scrapy edit <spider>

	指定したスパイダーを編集できるそうです。

- scrapy fetch <url>

	1つ選ぶ取ることをプログラム用語ではfetchをいうらしいです。URLを指定していますが、その内容をログに出力するようです。

- scrapy view <url>

	これを実行すると、スパイダー目線でブラウザが開きます。何個か試してみましたが、cssを読み込まないようですね。

- scrapy shell [url]

	shellは何回か実行しましたが、シェルでコードを試せます。

- scrapy parse <url> [options]

	指定したURLを取得し、それをスパイダーで処理・解析をするそうです。

- scrapy settings [options]

	スパイダーの設定をみれるそうです。オプションのところにスパイダーの名前を入れると、そのスパイダーの設定がみれます。
```
scrapy settings --get tutorial
None
```
何も設定してないので、Noneですね笑。

- scrapy runspider <spider_file.py>

	プロジェクトを作成せずに、Pythonでのスパイダーを実行します。

- scrapy version [-v]

	Scrapyのバージョンを出力します。

- scrapy bench

	ベンチマークテストを実行するそうです。	

ここまでコマンドのことを書く必要がなかったような気がしますが、明日からは実際にテキストを取得していきたいと思います。

##今日の結果
今日のAKBメンバーによる呟きは	37件でした。
彩さんや彩ちゃんといったように、山本彩の卒業に触れている呟きが多かったです。
[f:id:rimt:20180801022358p:plain]

どうでもいいですが、恋するフォーチュンクッキー のMusic Videoの3:57秒あたりの他のメンバーよりワンテンポ早く踊っている一瞬だけ映る山本彩が可愛いと思います（画像はないです）。

